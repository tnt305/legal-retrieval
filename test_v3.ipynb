{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing queries: 100%|██████████| 119456/119456 [00:00<00:00, 755357.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11308\n"
     ]
    }
   ],
   "source": [
    "from src.preprocessor.utils.dataset_level import read_pickle, prepare_training_dataset, read_json\n",
    "\n",
    "corpus = read_pickle('/home/thiendc/projects/legal_retrieval/data/processed/corpus.pkl')\n",
    "corpus =  dict(sorted(corpus.items(), key=lambda item: item[1])[:6000])\n",
    "corpus = {i: j.replace(\"\\xa0\", \"\") for i, j in corpus.items()}\n",
    "\n",
    "queries = read_pickle('/home/thiendc/projects/legal_retrieval/data/processed/queries.pkl')\n",
    "relevant_docs = read_pickle('/home/thiendc/projects/legal_retrieval/data/processed/relevant_docs.pkl')\n",
    "train_dataset = prepare_training_dataset(queries, corpus, relevant_docs)\n",
    "print(len(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 6461 new tokens to the vocabulary\n"
     ]
    }
   ],
   "source": [
    "# from sentence_transformers import SentenceTransformer, models\n",
    "# from transformers import AutoTokenizer, AutoModel\n",
    "# import os\n",
    "\n",
    "# def setup_embedding_model(model_name, new_tokens=None):\n",
    "#     \"\"\"\n",
    "#     Set up a sentence transformer model with proper tokenizer handling\n",
    "    \n",
    "#     Args:\n",
    "#         model_name (str): HuggingFace model name/path\n",
    "#         new_tokens (list): Optional list of new tokens to add to vocabulary\n",
    "    \n",
    "#     Returns:\n",
    "#         SentenceTransformer: Properly configured sentence transformer model\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Set up tokenizer and model in SentenceTransformer\n",
    "#     word_embedding_model = models.Transformer(model_name)\n",
    "#     tokenizer = word_embedding_model.tokenizer\n",
    "    \n",
    "#     # Add new tokens if provided\n",
    "#     if new_tokens is not None:\n",
    "#         tokenizer.add_tokens(new_tokens, special_tokens=False)\n",
    "#         word_embedding_model.auto_model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "#     # Create the SentenceTransformer model with the word embedding model\n",
    "#     sentence_model = SentenceTransformer(modules=[word_embedding_model])\n",
    "    \n",
    "#     return sentence_model\n",
    "\n",
    "# # Load new tokens and setup model\n",
    "# new_tokens = read_json('./src/preprocessor/vocab/data/update_vocab_v1.json') \n",
    "# tokenizer = AutoTokenizer.from_pretrained('intfloat/e5-base-v2')\n",
    "# tokenizer.add_tokens(new_tokens, special_tokens=False)\n",
    "\n",
    "# # 2. Create the base model\n",
    "# base_model = AutoModel.from_pretrained('intfloat/e5-base-v2')\n",
    "# base_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# # 3. Create word embedding model using sentence-transformers format\n",
    "# word_embedding_model = models.Transformer(\n",
    "#     model_name_or_path='intfloat/e5-base-v2',\n",
    "#     tokenizer_name_or_path='intfloat/e5-base-v2'\n",
    "# )\n",
    "\n",
    "# # 4. Create pooling model\n",
    "# pooling_model = models.Pooling(\n",
    "#     word_embedding_model.get_word_embedding_dimension(),\n",
    "#     pooling_mode_mean_tokens=True,\n",
    "#     pooling_mode_cls_token=False,\n",
    "#     pooling_mode_max_tokens=False\n",
    "# )\n",
    "\n",
    "# # 5. Create the full SentenceTransformer model\n",
    "# model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "# # Now you can use model directly in SentenceTransformerTrainer\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, models\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "def setup_embedding_model(model_name, new_tokens=None):\n",
    "    \"\"\"\n",
    "    Set up a sentence transformer model with proper tokenizer handling and pooling\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): HuggingFace model name/path\n",
    "        new_tokens (list): Optional list of new tokens to add to vocabulary\n",
    "    \n",
    "    Returns:\n",
    "        SentenceTransformer: Properly configured sentence transformer model\n",
    "    \"\"\"\n",
    "    # Set up word embedding model\n",
    "    word_embedding_model = models.Transformer(model_name)\n",
    "    tokenizer = word_embedding_model.tokenizer\n",
    "    \n",
    "    # Add new tokens if provided\n",
    "    if new_tokens is not None:\n",
    "        num_added_tokens = tokenizer.add_tokens(new_tokens, special_tokens=False)\n",
    "        print(f\"Added {num_added_tokens} new tokens to the vocabulary\")\n",
    "        # Resize model embeddings to account for new tokens\n",
    "        word_embedding_model.auto_model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    # Create pooling model\n",
    "    pooling_model = models.Pooling(\n",
    "        word_embedding_model.get_word_embedding_dimension(),\n",
    "        pooling_mode_mean_tokens=True,\n",
    "        pooling_mode_cls_token=False,\n",
    "        pooling_mode_max_tokens=False\n",
    "    )\n",
    "    \n",
    "    # Create the full SentenceTransformer model\n",
    "    sentence_model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "    \n",
    "    return sentence_model, tokenizer\n",
    "\n",
    "# Sử dụng hàm:\n",
    "# 1. Load new tokens\n",
    "new_tokens = read_json('./src/preprocessor/vocab/data/update_vocab_v2.json')\n",
    "\n",
    "# 2. Setup model với vocab mới\n",
    "model, _ = setup_embedding_model('VoVanPhuc/sup-SimCSE-VietNamese-phobert-base', new_tokens= new_tokens)\n",
    "# model = SentenceTransformer(\"intfloat/multilingual-e5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.evaluation import (\n",
    "    InformationRetrievalEvaluator,\n",
    "    SequentialEvaluator,\n",
    ")\n",
    "from sentence_transformers.util import cos_sim as consine\n",
    "\n",
    "\n",
    "matryoshka_dimensions = [768, 512, 256] # Important: large to small\n",
    "matryoshka_evaluators = []\n",
    "# Iterate over the different dimensions\n",
    "# for dim in matryoshka_dimensions:\n",
    "#     ir_evaluator = InformationRetrievalEvaluator(\n",
    "#         queries=queries,\n",
    "#         corpus=corpus,\n",
    "#         relevant_docs=relevant_docs,\n",
    "#         name=f\"dim_{dim}\",\n",
    "#         truncate_dim=dim,  # Truncate the embeddings to a certain dimension\n",
    "#         score_functions={\"cosine\": consine},\n",
    "#     )\n",
    "#     matryoshka_evaluators.append(ir_evaluator)\n",
    "\n",
    "# Create a sequential evaluator\n",
    "# evaluator = SequentialEvaluator(matryoshka_evaluators)\n",
    "evaluator = InformationRetrievalEvaluator(\n",
    "        queries=queries,\n",
    "        corpus=corpus,\n",
    "        relevant_docs=relevant_docs,\n",
    "        name=f\"dim_768\",\n",
    "        truncate_dim= 768,  # Truncate the embeddings to a certain dimension\n",
    "        score_functions={\"cosine\": consine},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sentence_transformers.losses import MatryoshkaLoss, MultipleNegativesRankingLoss\n",
    "\n",
    "inner_train_loss = MultipleNegativesRankingLoss(model)\n",
    "train_loss = MatryoshkaLoss(\n",
    "    model, inner_train_loss, matryoshka_dims=matryoshka_dimensions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Currently using DataParallel (DP) for multi-gpu training, while DistributedDataParallel (DDP) is recommended for faster training. See https://sbert.net/docs/sentence_transformer/training/distributed.html for more information.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "from contextlib import contextmanager\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "from sentence_transformers import SentenceTransformerTrainer, SentenceTransformerTrainingArguments\n",
    "\n",
    "@contextmanager\n",
    "def track_memory():\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    yield\n",
    "    print(f\"Peak memory: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "# Custom trainer với memory management\n",
    "class MemoryEfficientTrainer(SentenceTransformerTrainer):\n",
    "    def training_step(self, *args, **kwargs):\n",
    "        loss = super().training_step(*args, **kwargs)\n",
    "        \n",
    "        # Dọn memory Python và CUDA cache sau mỗi step\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        # Dọn memory sau mỗi epoch\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        super().on_epoch_end()\n",
    "\n",
    "# Training arguments\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=\"legal_finetuning\",\n",
    "    num_train_epochs = 1,\n",
    "    per_device_train_batch_size= 16,  # Giảm batch size             \n",
    "    gradient_accumulation_steps=8,  # Tăng gradient accumulation            \n",
    "    per_device_eval_batch_size= 16,\n",
    "    gradient_checkpointing=True,\n",
    "    warmup_ratio=0.15,\n",
    "    learning_rate=5e-6,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    fp16=True,\n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_steps= 20,\n",
    "    logging_steps = 5,\n",
    "    save_total_limit=5,\n",
    "    load_best_model_at_end=True,\n",
    "    max_grad_norm=0.5,\n",
    "    metric_for_best_model=\"eval_dim_768_cosine_ndcg@10\",\n",
    "    ddp_find_unused_parameters=False,\n",
    "    dataloader_num_workers = 40\n",
    ")\n",
    "\n",
    "# Khởi tạo trainer với custom class\n",
    "trainer = MemoryEfficientTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    loss=train_loss,\n",
    "    evaluator=evaluator,\n",
    ")\n",
    "\n",
    "# Dọn cache trước khi training\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainer = SentenceTransformerTrainer(\n",
    "#     model=model,\n",
    "#     args=args,\n",
    "#     train_dataset= train_dataset,\n",
    "#     loss=train_loss,\n",
    "#     evaluator=evaluator,\n",
    "# )\n",
    "# torch.cuda.empty_cache()\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mthiendc3005\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e06e8d999ea498892f23246e0883420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112643134159347, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/thiendc/projects/legal_retrieval/wandb/run-20241102_210546-z9wco0y6</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/thiendc3005/sentence-transformers/runs/z9wco0y6' target=\"_blank\">legal_finetuning</a></strong> to <a href='https://wandb.ai/thiendc3005/sentence-transformers' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/thiendc3005/sentence-transformers' target=\"_blank\">https://wandb.ai/thiendc3005/sentence-transformers</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/thiendc3005/sentence-transformers/runs/z9wco0y6' target=\"_blank\">https://wandb.ai/thiendc3005/sentence-transformers/runs/z9wco0y6</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='22' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 2/22 : < :, Epoch 0.05/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training với memory tracking\n",
    "with track_memory():\n",
    "    trainer.train()\n",
    "\n",
    "# Dọn memory sau khi training xong\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: write).\n",
      "Your token has been saved in your configured git credential helpers (store).\n",
      "Your token has been saved to /home/thiendc/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "588905a06ed849c6bdf96afe7df4e99d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/559M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/Tnt3o5/test_embedding_model/commit/a6177571d2c5535081604dbd0f8326683b8604f0'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\", add_to_git_credential=True)\n",
    "trainer.model.push_to_hub(\"test_embedding_model\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
