{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessor.utils.dataset_level import read_pickle, prepare_training_dataset_with_triplet, read_json\n",
    "from itertools import islice\n",
    "\n",
    "corpus = read_pickle('/home/thiendc/projects/legal_retrieval/data/processed/corpus.pkl')\n",
    "corpus = {i: j.replace(\"\\xa0\", \"\") for i, j in corpus.items()}\n",
    "\n",
    "queries = read_pickle('/home/thiendc/projects/legal_retrieval/data/processed/queries.pkl')\n",
    "# queries = dict(sorted(queries.items(), key=lambda item: item[1])[:1000])\n",
    "\n",
    "relevant_docs = read_pickle('/home/thiendc/projects/legal_retrieval/data/processed/relevant_docs.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing queries: 100%|██████████| 10000/10000 [00:00<00:00, 36231.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "532 5909 10000 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "selected_queries = dict(sorted(queries.items(), key=lambda item: item[1])[:10000])\n",
    "\n",
    "# Lọc corpus và relevant_docs dựa trên selected_queries\n",
    "selected_corpus = {i: corpus[i] for i in selected_queries.keys() if i in corpus}\n",
    "selected_relevant_docs = {i: relevant_docs[i] for i in selected_queries.keys() if i in relevant_docs}\n",
    "\n",
    "# Chuẩn bị dataset cho training\n",
    "train_dataset = prepare_training_dataset_with_triplet(selected_queries, selected_corpus, selected_relevant_docs)\n",
    "print(len(train_dataset), len(selected_corpus), len(selected_queries), len(selected_relevant_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing queries: 100%|██████████| 1000/1000 [00:00<00:00, 671518.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 596 1000 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "u_selected_queries = dict(sorted(queries.items(), key=lambda item: item[1])[-1000:])\n",
    "\n",
    "# Lọc corpus và relevant_docs dựa trên selected_queries\n",
    "u_selected_corpus = {i: corpus[i] for i in u_selected_queries.keys() if i in corpus}\n",
    "u_selected_relevant_docs = {i: relevant_docs[i] for i in u_selected_queries.keys() if i in relevant_docs}\n",
    "\n",
    "# Chuẩn bị dataset cho training\n",
    "val_dataset = prepare_training_dataset_with_triplet(u_selected_queries, u_selected_corpus, u_selected_relevant_docs)\n",
    "print(len(val_dataset), len(u_selected_corpus), len(u_selected_queries), len(u_selected_relevant_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 13295 new tokens to the vocabulary\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, models\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "def setup_embedding_model(model_name, new_tokens=None):\n",
    "    \"\"\"\n",
    "    Set up a sentence transformer model with proper tokenizer handling and pooling\n",
    "    \n",
    "    Args:\n",
    "        model_name (str): HuggingFace model name/path\n",
    "        new_tokens (list): Optional list of new tokens to add to vocabulary\n",
    "    \n",
    "    Returns:\n",
    "        SentenceTransformer: Properly configured sentence transformer model\n",
    "    \"\"\"\n",
    "    # Set up word embedding model\n",
    "    word_embedding_model = models.Transformer(model_name, max_seq_length= 512)\n",
    "    tokenizer = word_embedding_model.tokenizer\n",
    "    \n",
    "    # Add new tokens if provided\n",
    "    if new_tokens is not None:\n",
    "        num_added_tokens = tokenizer.add_tokens(new_tokens, special_tokens=False)\n",
    "        print(f\"Added {num_added_tokens} new tokens to the vocabulary\")\n",
    "        # Resize model embeddings to account for new tokens\n",
    "        word_embedding_model.auto_model.resize_token_embeddings(len(tokenizer))\n",
    "    \n",
    "    # Create pooling model\n",
    "    pooling_model = models.Pooling(\n",
    "        word_embedding_model.get_word_embedding_dimension(),\n",
    "        pooling_mode_mean_tokens=True,\n",
    "        pooling_mode_cls_token=False,\n",
    "        pooling_mode_max_tokens=False\n",
    "    )\n",
    "    \n",
    "    # Create the full SentenceTransformer model\n",
    "    sentence_model = SentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "    \n",
    "    return sentence_model, tokenizer\n",
    "\n",
    "# Sử dụng hàm:\n",
    "# 1. Load new tokens\n",
    "new_tokens = read_json('./src/preprocessor/vocab/data/update_vocab_v2.json')\n",
    "\n",
    "# 2. Setup model với vocab mới\n",
    "model, _ = setup_embedding_model('intfloat/multilingual-e5-small', new_tokens= new_tokens)\n",
    "# model = SentenceTransformer(\"intfloat/multilingual-e5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, models\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dangvantuan/vietnamese-embedding\", trust_remote_code = True)\n",
    "tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.evaluation import (\n",
    "    InformationRetrievalEvaluator,\n",
    "    SequentialEvaluator,\n",
    ")\n",
    "from sentence_transformers.util import cos_sim as consine\n",
    "\n",
    "\n",
    "matryoshka_dimensions = [384, 256, 128] # Important: large to small\n",
    "matryoshka_evaluators = []\n",
    "# Iterate over the different dimensions\n",
    "for dim in matryoshka_dimensions:\n",
    "    ir_evaluator = InformationRetrievalEvaluator(\n",
    "        queries=u_selected_queries,\n",
    "        corpus=u_selected_corpus,\n",
    "        relevant_docs=u_selected_relevant_docs,\n",
    "        name=f\"dim_{dim}\",\n",
    "        truncate_dim=dim,  # Truncate the embeddings to a certain dimension\n",
    "        score_functions={\"cosine\": consine},\n",
    "    )\n",
    "    matryoshka_evaluators.append(ir_evaluator)\n",
    "\n",
    "evaluator = SequentialEvaluator(matryoshka_evaluators)\n",
    "# evaluator = InformationRetrievalEvaluator(\n",
    "#         queries=queries,\n",
    "#         corpus=corpus,\n",
    "#         relevant_docs=relevant_docs,\n",
    "#         name=f\"dim_768\",\n",
    "#         truncate_dim= 768,  # Truncate the embeddings to a certain dimension\n",
    "#         score_functions={\"cosine\": consine},\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.losses import MatryoshkaLoss, MultipleNegativesRankingLoss\n",
    "\n",
    "inner_train_loss = MultipleNegativesRankingLoss(model)\n",
    "train_loss = MatryoshkaLoss(\n",
    "    model, inner_train_loss, matryoshka_dims=matryoshka_dimensions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Currently using DataParallel (DP) for multi-gpu training, while DistributedDataParallel (DDP) is recommended for faster training. See https://sbert.net/docs/sentence_transformer/training/distributed.html for more information.\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import gc\n",
    "from contextlib import contextmanager\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "from sentence_transformers import SentenceTransformerTrainer, SentenceTransformerTrainingArguments\n",
    "\n",
    "@contextmanager\n",
    "def track_memory():\n",
    "    torch.cuda.reset_peak_memory_stats()\n",
    "    yield\n",
    "    print(f\"Peak memory: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB\")\n",
    "\n",
    "# Custom trainer với memory management\n",
    "class MemoryEfficientTrainer(SentenceTransformerTrainer):\n",
    "    def training_step(self, *args, **kwargs):\n",
    "        loss = super().training_step(*args, **kwargs)\n",
    "        \n",
    "        # Dọn memory Python và CUDA cache sau mỗi step\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        return loss\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        # Dọn memory sau mỗi epoch\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "        super().on_epoch_end()\n",
    "\n",
    "# Training arguments\n",
    "args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=\"legal_finetuning_v4\",\n",
    "    num_train_epochs = 8,\n",
    "    per_device_train_batch_size= 4,  # Giảm batch size             \n",
    "    gradient_accumulation_steps= 4,  # Tăng gradient accumulation            \n",
    "    per_device_eval_batch_size= 8,\n",
    "    gradient_checkpointing=True,\n",
    "    warmup_ratio = 0.1,\n",
    "    learning_rate= 3e-5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim=\"adamw_torch_fused\",\n",
    "    fp16=True,\n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_steps= 32,\n",
    "    logging_steps = 8,\n",
    "    save_total_limit = 5,\n",
    "    load_best_model_at_end=True,\n",
    "    max_grad_norm = 0.5,\n",
    "    metric_for_best_model=\"eval_dim_768_cosine_ndcg@10\",\n",
    "    # resume_from_checkpoint = \"./legal_finetuning_v2/checkpoint-128\",\n",
    "    ddp_find_unused_parameters=False,\n",
    "    dataloader_num_workers = 40\n",
    ")\n",
    "\n",
    "# Khởi tạo trainer với custom class\n",
    "trainer = MemoryEfficientTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset= val_dataset,\n",
    "    loss=train_loss,\n",
    "    evaluator=evaluator,\n",
    ")\n",
    "\n",
    "# Dọn cache trước khi training\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# trainer = SentenceTransformerTrainer(\n",
    "#     model=model,\n",
    "#     args=args,\n",
    "#     train_dataset= train_dataset,\n",
    "#     loss=train_loss,\n",
    "#     evaluator=evaluator,\n",
    "# )\n",
    "# torch.cuda.empty_cache()\n",
    "# trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='64' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 5/64 05:19 < 1:44:53, 0.01 it/s, Epoch 0.47/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training với memory tracking\n",
    "with track_memory():\n",
    "    trainer.train()\n",
    "\n",
    "# Dọn memory sau khi training xong\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: write).\n",
      "Your token has been saved in your configured git credential helpers (store).\n",
      "Your token has been saved to /home/thiendc/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ba651845e09481e817e38cf40147ab0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdf352e40db5492a896c1888f5b1ec6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fa4dc3a2c78483fb2c271308f8d741f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/560M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/Tnt3o5/sup_legal_phbert_triplet/commit/9f15d5377c19897d7d359f7dbedf037ab08b14c9'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\", add_to_git_credential=True)\n",
    "trainer.model.push_to_hub(\"sup_legal_phbert_triplet\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
