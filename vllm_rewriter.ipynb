{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessor.utils.dataset_level import read_pickle, prepare_training_dataset, read_json\n",
    "from src.generator.gen_label.settings import REWRITING_PROMPT\n",
    "# from src.generator.gen_label.competion import qwen_completion_to_prompt\n",
    "# from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download\n",
    "model_path = snapshot_download(\n",
    "    repo_id=\"bert-base-uncased\",\n",
    "    local_dir=\"./my_model\"  # Thư mục để lưu model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 11-07 09:31:40 config.py:1668] Casting torch.bfloat16 to torch.float16.\n",
      "INFO 11-07 09:32:12 config.py:905] Defaulting to use mp for distributed inference\n",
      "INFO 11-07 09:32:12 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='Qwen/Qwen2-1.5B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=2, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Qwen/Qwen2-1.5B-Instruct, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)\n",
      "WARNING 11-07 09:32:12 multiproc_gpu_executor.py:53] Reducing Torch parallelism from 24 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "INFO 11-07 09:32:12 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n",
      "INFO 11-07 09:32:13 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 11-07 09:32:13 selector.py:115] Using XFormers backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thiendc/projects/.thienenv/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "/home/thiendc/projects/.thienenv/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "  @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=3074092)\u001b[0;0m INFO 11-07 09:32:31 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3074092)\u001b[0;0m INFO 11-07 09:32:31 selector.py:115] Using XFormers backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=3074092)\u001b[0;0m /home/thiendc/projects/.thienenv/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:211: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3074092)\u001b[0;0m   @torch.library.impl_abstract(\"xformers_flash::flash_fwd\")\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3074092)\u001b[0;0m /home/thiendc/projects/.thienenv/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:344: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3074092)\u001b[0;0m   @torch.library.impl_abstract(\"xformers_flash::flash_bwd\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=3074092)\u001b[0;0m INFO 11-07 09:32:49 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n",
      "INFO 11-07 09:32:50 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "INFO 11-07 09:32:50 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3074092)\u001b[0;0m INFO 11-07 09:32:50 utils.py:1008] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3074092)\u001b[0;0m INFO 11-07 09:32:50 pynccl.py:63] vLLM is using nccl==2.20.5\n",
      "INFO 11-07 09:32:50 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/thiendc/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3074092)\u001b[0;0m WARNING 11-07 09:32:50 custom_all_reduce.py:141] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 11-07 09:32:50 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/thiendc/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3074092)\u001b[0;0m WARNING 11-07 09:32:50 custom_all_reduce.py:141] Custom allreduce is disabled because your platform lacks GPU P2P capability or P2P test failed. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "INFO 11-07 09:32:50 shm_broadcast.py:241] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7efec8575390>, local_subscribe_port=41187, remote_subscribe_port=None)\n",
      "INFO 11-07 09:32:50 model_runner.py:1056] Starting to load model Qwen/Qwen2-1.5B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3074092)\u001b[0;0m INFO 11-07 09:32:50 model_runner.py:1056] Starting to load model Qwen/Qwen2-1.5B-Instruct...\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3074092)\u001b[0;0m INFO 11-07 09:32:50 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3074092)\u001b[0;0m INFO 11-07 09:32:50 selector.py:115] Using XFormers backend.\n",
      "INFO 11-07 09:32:50 selector.py:224] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
      "INFO 11-07 09:32:50 selector.py:115] Using XFormers backend.\n",
      "INFO 11-07 09:32:51 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3074092)\u001b[0;0m INFO 11-07 09:32:51 weight_utils.py:243] Using model weights format ['*.safetensors']\n",
      "INFO 11-07 09:32:51 weight_utils.py:288] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ec6fc3487b24d559a2614007d1caada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(VllmWorkerProcess pid=3074092)\u001b[0;0m INFO 11-07 09:32:52 weight_utils.py:288] No model.safetensors.index.json found in remote.\n",
      "INFO 11-07 09:32:53 model_runner.py:1067] Loading model weights took 1.4478 GB\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3074092)\u001b[0;0m INFO 11-07 09:32:53 model_runner.py:1067] Loading model weights took 1.4478 GB\n",
      "INFO 11-07 09:32:58 distributed_gpu_executor.py:57] # GPU blocks: 27530, # CPU blocks: 18724\n",
      "INFO 11-07 09:32:58 distributed_gpu_executor.py:61] Maximum concurrency for 32768 tokens per request: 13.44x\n",
      "INFO 11-07 09:33:02 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 11-07 09:33:02 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3074092)\u001b[0;0m INFO 11-07 09:33:03 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3074092)\u001b[0;0m INFO 11-07 09:33:03 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[1;36m(VllmWorkerProcess pid=3074092)\u001b[0;0m INFO 11-07 09:33:19 model_runner.py:1523] Graph capturing finished in 17 secs.\n",
      "INFO 11-07 09:33:19 model_runner.py:1523] Graph capturing finished in 17 secs.\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(temperature=0.5, top_p=0.95, max_tokens = 256)\n",
    "llm = LLM(model=\"Qwen/Qwen2-1.5B-Instruct\", dtype = 'float16', tensor_parallel_size = 2, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2queries = read_pickle('/home/thiendc/projects/legal_retrieval/data/processed/queries.pkl')\n",
    "special_queries = []\n",
    "for i in list(id2queries.values()):\n",
    "    if i.count(\"?\") >1:\n",
    "        special_queries.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Phẫu thuật viên có thể thực hiện phẫu thuật điều trị hở mi không? Người bệnh được chỉ định phẫu thuật điều trị hở mi khi nào?'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "REWRITING_PROMPT = \"\"\"Hãy viết lại câu hỏi truy vấn đầu vào thành một câu hỏi DUY NHẤT có định dạng json:\n",
    "\n",
    "{\"rewriter\": Câu hỏi truy vấn được viết lại}\n",
    "\n",
    "Yêu cầu:\n",
    "- Reformulation: Viết lại query với cách diễn đạt khác\n",
    "- Câu viết lại chỉ được phép là câu hỏi có DUY NHẤT 1 dấu hỏi chấm ( ? )\n",
    "\"\"\"\n",
    "\n",
    "special_queries[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.33it/s, est. speed input: 301.07 toks/s, output: 88.68 toks/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"rewriter\": Phẫu thuật viên có thể thực hiện phẫu thuật điều trị hở mi không? Người bệnh được chỉ định phẫu thuật điều trị hở mi khi nào?}'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampling_params = SamplingParams(temperature=0.8, top_p=0.95, max_tokens = 256)\n",
    "prompt = qwen_completion_to_prompt(REWRITING_PROMPT, special_queries[5])\n",
    "llm.generate(prompt, sampling_params)[0].outputs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Để xác định giá cước viễn thông thì cần tuân thủ những nguyên tắc gì? Việc xác định giá cước được dựa trên những căn cứ nào?'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = special_queries[2]\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen2.5-1.5B-Instruct\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "REWRITING_PROMPT = \"\"\"Bạn là tư vấn viên pháp luật, hữu ích và giỏi giang về pháp luật dân sự Việt Nam.\n",
    "Câu hỏi dưới đây đang có hai câu hỏi ( mỗi câu hỏi được tách nhau bởi dấu hỏi chấm)\n",
    "Hãy tóm tắt câu hỏi truy vấn đầu vào thành một câu hỏi DUY NHẤT có định dạng json:\n",
    "\n",
    "{\"rewriter\": Câu hỏi truy vấn được viết lại}\n",
    "\n",
    "Yêu cầu đầu ra là\n",
    "- Viết lại query với cách diễn đạt khác\n",
    "- Làm rõ ý định tìm kiếm\n",
    "- Câu viết lại chỉ được phép là câu tồn tại DUY NHẤT 1 dấu hỏi chấm ( ? ).\n",
    "\n",
    "Bạn không được phép trả lời câu hỏi mà chỉ được tóm tắt để hỏi một lần duy nhất. Một câu hỏi phải kết thúc bằng dấu hỏi chấm (? )\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": REWRITING_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens= 64\n",
    ")\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"rewriter\":\"Vì sao cần tuân thủ các nguyên tắc khi xác định giá cước viễn thông?\"}'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|im_start|> Bạn là thẩm phán có hiểu biết sâu rộng về pháp luật dân sự Việt Nam. Hãy viết lại câu truy vấn đầu vào, trong đó:\\n\\nCâu đầu vào của tôi hiện đang có hai câu hỏi, được tách nhau bởi dấu hỏi chấm (?). \\nXem xét mối quan hệ giữa các câu hỏi, được tách nhau bởi dấu ? để để viết lại câu trên chỉ dùng 1 câu để hỏi.\\nBiết rằng rằng câu được viết lại phải là một câu hỏi chứ không phải hai câu trở lên.\\n\\nKhông được phép viết lại đây là câu hỏi mà chỉ viết lại câu hỏi ban đầu thành câu hỏi và format dưới dạng json. \\n{ \"rewriter\": \"Câu hỏi được tổng hợp và hỏi lại chỉ sử dụng 1 câu\" }\\n\\nHãy viết lại với câu hỏi truy vấn sau:\\n \\n<|im_end|>\\n<|im_start|>user\\nPhẫu thuật viên có thể thực hiện phẫu thuật điều trị hở mi không? Người bệnh được chỉ định phẫu thuật điều trị hở mi khi nào?<|im_end|>\\n<|im_start|>assistant\\n'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
