training:
  output_dir: "legal_finetune"
  num_train_epochs: 10
  per_device_train_batch_size: 16
  gradient_accumulation_steps: 4
  per_device_eval_batch_size: 16
  gradient_checkpointing: true
  warmup_ratio: 0.1
  learning_rate: 2e-5
  lr_scheduler_type: "cosine"
  optim: "adamw_torch_fused"
  fp16: true
  batch_sampler: "NO_DUPLICATES"
  eval_strategy: "steps"
  save_steps: 500
  logging_steps: 10
  save_total_limit: 3
  load_best_model_at_end: true
  max_grad_norm: 1.0
  metric_for_best_model: "eval_dim_768_cosine_ndcg@10"
  dataloader_pin_memory: false
  dataloader_num_workers: 4

environment:
  PYTORCH_CUDA_ALLOC_CONF: "max_split_size_mb:512"
