{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"./data/update_vocab.json\", 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "import re\n",
    "import ast\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from typing import List, Set, Dict, Union, Tuple, Optional\n",
    "from src.preprocessor.utils.sent_level import remove_punc, remove_n_items\n",
    "from src.preprocessor.vocab.stopwords import STOP_WORDS\n",
    "from src.preprocessor.vocab.legal_dict import LEGAL_DICT\n",
    "from src.preprocessor.vocab.duties_dict import DUTIES\n",
    "from src.preprocessor.vocab.special_terms import SPECIAL_TERMS\n",
    "from src.preprocessor.vocab.acronym import ACRONYMS\n",
    "from src.preprocessor.vocab.province import PROVINCES\n",
    "from src.preprocessor.vocab.roman_numerals_dict import ROMAN_DICT\n",
    "from src.preprocessor.base.base_model import BaseTextPostPreprocessor\n",
    "\n",
    "\n",
    "class PostPreprocessing(BaseTextPostPreprocessor):\n",
    "    \"\"\"Class for post-processing text with various rules and vocabulary handling.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        legal_term: Optional[Dict[str, str]] = None,\n",
    "        stop_words: Optional[Set[str]] = None,\n",
    "        duty_term: Optional[Dict[str, str]] = None,\n",
    "        special_term: Optional[Set[str]] = None\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize PostPreprocessing object with vocabularies and terms.\n",
    "        \n",
    "        Args:\n",
    "            legal_term: Dictionary of legal terms and definitions\n",
    "            stop_words: Set of stop words to be filtered\n",
    "            duty_term: Dictionary of duties and their definitions\n",
    "            special_term: Set of special terms\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.legal_term = legal_term or LEGAL_DICT\n",
    "        self.stop_words = stop_words or STOP_WORDS\n",
    "        self.duties = duty_term or DUTIES\n",
    "        self.special_terms = special_term or SPECIAL_TERMS\n",
    "        \n",
    "        # Create token sets for faster lookup\n",
    "        self.legal_tokens = set(self.legal_term.values())\n",
    "        self.stopwords_tokens = {i for i in self.stop_words if i}\n",
    "        self.duties_tokens = set(self.duties.values())\n",
    "        self.special_tokens = set(self.special_terms)\n",
    "\n",
    "    @staticmethod\n",
    "    def handle_n_items(text: str) -> str:\n",
    "        \"\"\"Handle items starting with 'n' followed by numbers.\n",
    "    \n",
    "        Args:\n",
    "            text: Input text to process\n",
    "            \n",
    "        Returns:\n",
    "            Processed text with n-items handled\n",
    "        \"\"\"\n",
    "        numbers = map(str, range(1, 10))\n",
    "        rewrite_text = []\n",
    "        words = text.split(\"_\")\n",
    "        for word in words:\n",
    "            if word.startswith('n'):\n",
    "                if len(word) >= 2:\n",
    "                    if word[1] == word[1].upper():\n",
    "                        if any(num in word[1] for num in numbers): # word[2]\n",
    "                            rewrite_text.append(word[2:])\n",
    "                        else:\n",
    "                            rewrite_text.append(word[1:])  \n",
    "                    else:\n",
    "                        rewrite_text.append(word)\n",
    "                else:\n",
    "                    rewrite_text.append(word)\n",
    "            else:\n",
    "                rewrite_text.append(word)\n",
    "        \n",
    "        rewrite_text = \"_\".join(rewrite_text)\n",
    "\n",
    "        return rewrite_text\n",
    "\n",
    "    @staticmethod\n",
    "    def handle_rules(text: str):\n",
    "        \"\"\"Handle rules based text processing with specific formatting.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text containing rules\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of processed text parts or original text if no rules apply\n",
    "        \"\"\"\n",
    "        numbers = [str(i) for i in range(1, 1000)]\n",
    "        rules = ['Khoản', 'Điều', 'Điểm', 'Chương', 'Cấp', 'Hạng', 'Mục']\n",
    "        words = text.split(\"_\")\n",
    "        \n",
    "        if len(words) == 4 and any(rule in text for rule in rules):\n",
    "            return f\"{words[0]}_{words[1]}\", f\"{words[-2]}_{words[-1]}\"\n",
    "        \n",
    "        elif len(words) == 6 and any(rule in text for rule in rules):\n",
    "            return (f\"{words[0]}_{words[1]}\", \n",
    "                   f\"{words[2]}_{words[3]}\", \n",
    "                   f\"{words[-2]}_{words[-1]}\")\n",
    "        \n",
    "        elif len(words) > 2 and words[-2] in rules and words[-1] in numbers:\n",
    "            return (f\"{words[-2]}_{words[-1]}\",\n",
    "                f\"{words[:-2]}\",)\n",
    "        else:\n",
    "            return text\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def handle_punc(text: str) -> str:\n",
    "        \"\"\"Remove trailing periods from text.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text\n",
    "            \n",
    "        Returns:\n",
    "            Text with trailing periods removed\n",
    "        \"\"\"\n",
    "        text = text.strip(\".\")\n",
    "        text =  text.replace(\".\", \"\")\n",
    "        return text\n",
    "\n",
    "    def handle_rules_v2(self, text: str) -> Union[Tuple[str, str], str]:\n",
    "        \"\"\"Handle additional vocabulary-based rules.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of matched term and remaining text, or single term if no match\n",
    "        \"\"\"\n",
    "        for term in self.duties_tokens:\n",
    "            if term in text:\n",
    "                sub_text = text.split(term)[-1].strip(\"_\")\n",
    "                if (sub_text in self.duties_tokens or \n",
    "                    sub_text in self.special_tokens or \n",
    "                    sub_text in self.legal_tokens):\n",
    "                    return term, sub_text\n",
    "                return term\n",
    "        return text\n",
    "\n",
    "    @staticmethod\n",
    "    def handle_xa0_and_stopwords(text: str) -> str:\n",
    "        \"\"\"Remove 'xa0' characters and handle stopwords.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text\n",
    "            \n",
    "        Returns:\n",
    "            Processed text with xa0 removed and stopwords handled\n",
    "        \"\"\"\n",
    "        new_text = text.replace('xa0', '')\n",
    "        new_text = text.replace('xAA0', '')\n",
    "        words = text.split(\"_\")\n",
    "        \n",
    "        if words[-1].lower() in list(STOP_WORDS):\n",
    "            new_text = \"_\".join(words[:-1])\n",
    "        \n",
    "        if len(words) > 2 and len(words) % 2 == 1 and 'trừ' in words[-1]:\n",
    "            new_text = \"_\".join(words[:-1])\n",
    "            \n",
    "        return new_text\n",
    "\n",
    "    @staticmethod\n",
    "    def handle_uppercase(text: str) -> str:\n",
    "        \"\"\"Normalize text case based on specific rules.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text\n",
    "            \n",
    "        Returns:\n",
    "            Text with normalized case\n",
    "        \"\"\"\n",
    "        if isinstance(text, int):\n",
    "            return \"\"\n",
    "        elif isinstance(text, float):\n",
    "            return \"\"\n",
    "        elif text.isupper():\n",
    "            text = text.lower()\n",
    "            words = text.split(\"_\")\n",
    "            new_words = [\n",
    "                word.lower() if any(char.isupper() for char in word[1:])\n",
    "                else word for word in words\n",
    "            ]\n",
    "            return \"_\".join(new_words)\n",
    "        else:\n",
    "            return text\n",
    "\n",
    "    @staticmethod\n",
    "    def handle_acronym(text: str) -> str:\n",
    "        \"\"\"Replace acronyms with their full forms if available.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text\n",
    "            \n",
    "        Returns:\n",
    "            Text with acronyms replaced\n",
    "        \"\"\"\n",
    "        return ACRONYMS.get(text, text)\n",
    "\n",
    "    @staticmethod\n",
    "    def handle_numerical(text: str) -> Union[int, str]:\n",
    "        \"\"\"Convert text to integer if possible.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text\n",
    "            \n",
    "        Returns:\n",
    "            Integer if conversion successful, original text otherwise\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return int(text)\n",
    "        except ValueError:\n",
    "            return text\n",
    "\n",
    "    @staticmethod\n",
    "    def handle_places(text: str) -> str:\n",
    "        pass \n",
    "    \n",
    "    @staticmethod\n",
    "    def normalize_text(text: str) -> str:\n",
    "        norm_words = []\n",
    "        if any(province in text for province in list(PROVINCES.values())) or \\\n",
    "            any(province in text for province in list(DUTIES)) or \\\n",
    "                any(province in text for province in list(ROMAN_DICT)) or \\\n",
    "                    any(province in text for province in ['Khoản', 'Điều', 'Điểm', 'Chương', 'Cấp', 'Hạng', 'Mục']):\n",
    "            return text\n",
    "        else:\n",
    "            words = text.split('_')\n",
    "            # Chuyển từng từ thành chữ thường\n",
    "            for word in words:\n",
    "                if word not in set(PROVINCES.values()) and word not in DUTIES and word not in ROMAN_DICT:\n",
    "                    word = word.lower()\n",
    "                else:\n",
    "                    word = word\n",
    "                norm_words.append(word)\n",
    "            \n",
    "            # Ghép lại thành chuỗi với dấu gạch dưới\n",
    "            normalized_text = '_'.join(norm_words)\n",
    "            return normalized_text\n",
    "   \n",
    "    def post_preprocess_text(\n",
    "        self, \n",
    "        text: str\n",
    "    ) -> Union[str, Tuple[str, str], Tuple[str, str, str], int]:\n",
    "        \"\"\"Apply all post-processing steps to input text.\n",
    "        \n",
    "        Args:\n",
    "            text: Input text to process\n",
    "            \n",
    "        Returns:\n",
    "            Processed text in various formats depending on applied rules\n",
    "        \"\"\"\n",
    "        final_text = []\n",
    "        text = self.handle_punc(text)\n",
    "        text = self.handle_n_items(text)\n",
    "        text = self.handle_xa0_and_stopwords(text)\n",
    "        text = self.handle_numerical(text)\n",
    "        text = self.handle_uppercase(text)\n",
    "        text = self.handle_acronym(text)\n",
    "        texts = self.handle_rules(text)\n",
    "        if isinstance(texts, tuple):\n",
    "            for i in range(len(texts)):\n",
    "                txt = self.handle_rules_v2(texts[i])\n",
    "                txt = self.normalize_text(txt)\n",
    "                \n",
    "                final_text.append(txt)\n",
    "        else:\n",
    "            texts = self.normalize_text(texts)\n",
    "            final_text.append(texts)\n",
    "        return final_text\n",
    "    \n",
    "    def post_preprocess(self, \n",
    "                        docs: Union[List[str], pd.Series, str]):\n",
    "        \"\"\"\n",
    "        DO NOT USE THIS\n",
    "        \"\"\"\n",
    "        if isinstance(docs, str):\n",
    "            return self.post_preprocess_text(docs)\n",
    "        elif isinstance(docs, list):\n",
    "            return [self.post_preprocess_text(doc) for doc in tqdm(docs)]\n",
    "        else:  # pandas Series\n",
    "            tqdm.pandas()\n",
    "            return docs.progress_apply(self.post_preprocess_text)\n",
    "    \n",
    "    def post_preprocess_v1(self, docs: List):\n",
    "        v1_processed = []\n",
    "        for i in range(len(docs)):\n",
    "            v1_processed.extend(self.post_preprocess_text(docs[i]))\n",
    "        return list(set(v1_processed))\n",
    "        \n",
    "    def post_preprocess_v2(self, docs: List):\n",
    "        v2_processed = set()\n",
    "        v1_processed = set(self.post_preprocess_v1(docs))\n",
    "\n",
    "        for item in v1_processed:\n",
    "            item_lower = item.lower()\n",
    "            \n",
    "            # Kiểm tra duties\n",
    "            matching_duties = {duty for duty in DUTIES.values() if duty.lower() in item_lower}\n",
    "            if matching_duties:\n",
    "                v2_processed.update(matching_duties)\n",
    "                continue\n",
    "            \n",
    "            # Xử lý item có 4 phần\n",
    "            parts = item.split(\"_\")\n",
    "            if len(parts) == 4:\n",
    "                v2_processed.update([f\"{parts[0]}_{parts[1]}\", f\"{parts[-2]}_{parts[-1]}\"])\n",
    "            else:\n",
    "                v2_processed.add(ACRONYMS.get(item, item))\n",
    "\n",
    "        # Áp dụng các regex transformation, loại bỏ stop words và datetime\n",
    "        v2_processed = {\n",
    "            \"\" if (\n",
    "                item.lower() in STOP_WORDS or \n",
    "                re.match(r'\\d{1,2}/\\d{1,2}/\\d{4}', item)\n",
    "            ) else re.sub(r'\\b\\w_\\w\\b', '', item).replace('xad', '')\n",
    "            for item in v2_processed\n",
    "        }\n",
    "\n",
    "        # Loại bỏ chuỗi rỗng khỏi set\n",
    "        v2_processed.discard(\"\")\n",
    "        \n",
    "        processed_data = []\n",
    "        for item in v2_processed:\n",
    "            if \"[\" in item or \"]\" in item:\n",
    "                item = ast.literal_eval(item)\n",
    "                if len(item) % 2 == 0 and len(item) > 2:\n",
    "                    for i in range(0, len(item), 2):\n",
    "                        processed_data.append(f\"{item[i]}_{item[i+1]}\")\n",
    "                else:\n",
    "                    processed_data.append(item[0])\n",
    "            else:\n",
    "                processed_data.append(item)\n",
    "        \n",
    "        return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "postprocessor = PostPreprocessing()\n",
    "new_corpus = postprocessor.post_preprocess_v2(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.preprocessor.utils.dataset_level import save_json\n",
    "\n",
    "save_json(list(new_corpus), './data/update_vocab.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
